#reconstruction

# Self-Supervised Deep Equilibrium Models for Inverse Problems with Theoretical Guarantees
Weijie Gan, Chunwei Ying, Parna Eshraghi, Tongyao Wang, Cihat Eldeniz, Yuyang Hu, Jiaming Liu, Yasheng Chen, Hong An, Ulugbek Kamilov
## Setup
Image formation model: $y = MAx + e$
- $x$ is the underlying image
- $y$ is the acquired data
- $A$ is the forward operator that measures data
- $M$ is the undersampling matrix
- $e$ is Gaussian noise

Ill-posed image optimization solves $\hat x = \arg\min_x f(x)$
- $f(x)$ is a loss function
- decompose $f(x) = g(x) + h(x)$
	- $g(x)$ is a data-fidelity term, measures discrepancy between $y$ and predicted $\hat y = A \hat x$
	- $h(x)$ is a regularizer imposing image prior on $\hat x$
## Deep Unrolling
Inspired by [Model-based Deep Learning Architectures for Inverse Problems](https://github.com/hkaggarwal/modl/tree/master)
- Assume the forward operator and the acquired data are known ($A$ and $y$)
- Adapt the loss function $f(x)$ to be $$\hat{x}=\arg\min_{x} \frac{1}{2} \|y-Ax\|_2^2 + r(x)$$
- The gradient wrt $x$ is $$-A^T(y-Ax) + \nabla r(x)$$
- If we start with an initial guess for $x$ (say $x^0 = A^*y$), we can update with gradient descent:$$x^{k+1} = x^k +  \eta A^T(y - Ax^k) - \eta \nabla r(x^k)$$
- In *deep* unrolling, the deterministic function $r$ can be replaced with a deep network $R_\theta$
- The iterations $k=1,\dots,K$ typically ranges from 5-10, this is kept small because of issues with numerical stability, memory constraints, and vanishing gradients in backpropagation (each network call increases memory requirements linearly)
## Deep Equilibrium Models in Inverse Imaging
- Very similar to Deep Unrolling, but now a network $f_\theta(y)=x^0$ and $f_{\theta}(x^k)=x^{k+1}$ (QUESTION: how does this work for $x$ and $y$ of different dimensions)
- As $k \to \infty$, $x^k$ becomes a fixed point of the network
- To simplify issues with Deep Unrolling, the original Deep Equilibrium paper ([[23.08.30]]) makes $f_\theta$ a single-layer network, which they show can be learned with constant memory using implicit differentiation